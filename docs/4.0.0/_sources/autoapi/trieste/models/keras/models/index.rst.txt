:orphan:

:py:mod:`trieste.models.keras.models`
=====================================

.. py:module:: trieste.models.keras.models


Module Contents
---------------

.. py:class:: DeepEnsemble(model: trieste.models.keras.architectures.KerasEnsemble, optimizer: Optional[trieste.models.optimizer.KerasOptimizer] = None, bootstrap: bool = False, diversify: bool = False, continuous_optimisation: bool = True, compile_args: Optional[Mapping[str, Any]] = None, encoder: trieste.space.EncoderFunction | None = None)


   Bases: :py:obj:`trieste.models.keras.interface.KerasPredictor`, :py:obj:`trieste.models.interfaces.EncodedTrainableProbabilisticModel`, :py:obj:`trieste.models.keras.interface.DeepEnsembleModel`, :py:obj:`trieste.models.interfaces.HasTrajectorySampler`

   A :class:`~trieste.model.TrainableProbabilisticModel` wrapper for deep ensembles built using
   Keras.

   Deep ensembles are ensembles of deep neural networks that have been found to have good
   representation of uncertainty in practice (<cite data-cite="lakshminarayanan2017simple"/>).
   This makes them a potentially attractive model for Bayesian optimization for use-cases with
   large number of observations, non-stationary objective functions and need for fast predictions,
   in which standard Gaussian process models are likely to struggle. The model consists of simple
   fully connected multilayer probabilistic networks as base learners, with Gaussian distribution
   as a final layer, using the negative log-likelihood loss for training the networks. The
   model relies on differences in random initialization of weights for generating diversity among
   base learners.

   The original formulation of the model does not include boostrapping of the data. The authors
   found that it does not improve performance the model. We include bootstrapping as an option
   as later work that more precisely measured uncertainty quantification found that boostrapping
   does help with uncertainty representation (see <cite data-cite="osband2021epistemic"/>).

   We provide classes for constructing ensembles using Keras
   (:class:`~trieste.models.keras.KerasEnsemble`) in the `architectures` package that should be
   used with the :class:`~trieste.models.keras.DeepEnsemble` wrapper. There we also provide a
   :class:`~trieste.models.keras.GaussianNetwork` base learner following the original
   formulation in <cite data-cite="lakshminarayanan2017simple"/>, but any user-specified network
   can be supplied, as long as it has a Gaussian distribution as a final layer and follows the
   :class:`~trieste.models.keras.KerasEnsembleNetwork` interface.

   A word of caution in case a learning rate scheduler is used in ``fit_args`` to
   :class:`KerasOptimizer` optimizer instance. Typically one would not want to continue with the
   reduced learning rate in the subsequent Bayesian optimization step. Hence, we reset the
   learning rate to the original one after calling the ``fit`` method. In case this is not the
   behaviour you would like, you will need to subclass the model and overwrite the
   :meth:`optimize` method.

   Currently, we do not support setting up the model with dictionary config.

   :param model: A Keras ensemble model with probabilistic networks as ensemble members. The
       model has to be built but not compiled.
   :param optimizer: The optimizer wrapper with necessary specifications for compiling and
       training the model. Defaults to :class:`~trieste.models.optimizer.KerasOptimizer` with
       :class:`~tf.optimizers.Adam` optimizer, negative log likelihood loss, mean squared
       error metric and a dictionary of default arguments for Keras `fit` method: 3000 epochs,
       batch size 16, early stopping callback with patience of 50, and verbose 0.
       See https://keras.io/api/models/model_training_apis/#fit-method for a list of possible
       arguments.
   :param bootstrap: Sample with replacement data for training each network in the ensemble.
       By default, set to `False`.
   :param diversify: Whether to use quantiles from the approximate Gaussian distribution of
       the ensemble as trajectories instead of mean predictions when calling
       :meth:`trajectory_sampler`. This mode can be used to increase the diversity
       in case of optimizing very large batches of trajectories. By
       default, set to `False`.
   :param continuous_optimisation: If True (default), the optimizer will keep track of the
       number of epochs across BO iterations and use this number as initial_epoch. This is
       essential to allow monitoring of model training across BO iterations.
   :param compile_args: Keyword arguments to pass to the ``compile`` method of the
       Keras model (:class:`~tf.keras.Model`).
       See https://keras.io/api/models/model_training_apis/#compile-method for a
       list of possible arguments. The ``optimizer``, ``loss`` and ``metrics`` arguments
       must not be included.
   :param encoder: Optional encoder with which to transform query points before
       generating predictions.
   :raise ValueError: If ``model`` is not an instance of
       :class:`~trieste.models.keras.KerasEnsemble`, or ensemble has less than two base
       learners (networks), or `compile_args` contains disallowed arguments.

   .. py:property:: model
      :type: tensorflow.keras.Model

      Returns compiled Keras ensemble model.


   .. py:property:: ensemble_size
      :type: int

      Returns the size of the ensemble, that is, the number of base learners or individual neural
      network models in the ensemble.


   .. py:property:: num_outputs
      :type: int

      Returns the number of outputs trained on by each member network.


   .. py:property:: dtype
      :type: tensorflow.DType

      The prediction dtype.


   .. py:method:: prepare_dataset(dataset: trieste.data.Dataset) -> tuple[Dict[str, trieste.types.TensorType], Dict[str, trieste.types.TensorType]]

      Transform ``dataset`` into inputs and outputs with correct names that can be used for
      training the :class:`KerasEnsemble` model.

      If ``bootstrap`` argument in the :class:`~trieste.models.keras.DeepEnsemble` is set to
      `True`, data will be additionally sampled with replacement, independently for
      each network in the ensemble.

      :param dataset: A dataset with ``query_points`` and ``observations`` tensors.
      :return: A dictionary with input data and a dictionary with output data.


   .. py:method:: prepare_query_points(query_points: trieste.types.TensorType) -> Dict[str, trieste.types.TensorType]

      Transform ``query_points`` into inputs with correct names that can be used for
      predicting with the model.

      :param query_points: A tensor with ``query_points``.
      :return: A dictionary with query_points prepared for predictions.


   .. py:method:: ensemble_distributions(query_points: trieste.types.TensorType) -> tuple[tensorflow_probability.python.distributions.Distribution, Ellipsis]

      Return distributions for each member of the ensemble.

      :param query_points: The points at which to return distributions.
      :return: The distributions for the observations at the specified
          ``query_points`` for each member of the ensemble.


   .. py:method:: predict_encoded(query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Returns mean and variance at ``query_points`` for the whole ensemble.

      Following <cite data-cite="lakshminarayanan2017simple"/> we treat the ensemble as a
      uniformly-weighted Gaussian mixture model and combine the predictions as

      .. math:: p(y|\mathbf{x}) = M^{-1} \Sum_{m=1}^M \mathcal{N}
          (\mu_{\theta_m}(\mathbf{x}),\,\sigma_{\theta_m}^{2}(\mathbf{x}))

      We further approximate the ensemble prediction as a Gaussian whose mean and variance
      are respectively the mean and variance of the mixture, given by

      .. math:: \mu_{*}(\mathbf{x}) = M^{-1} \Sum_{m=1}^M \mu_{\theta_m}(\mathbf{x})

      .. math:: \sigma^2_{*}(\mathbf{x}) = M^{-1} \Sum_{m=1}^M (\sigma_{\theta_m}^{2}(\mathbf{x})
          + \mu^2_{\theta_m}(\mathbf{x})) - \mu^2_{*}(\mathbf{x})

      This method assumes that the final layer in each member of the ensemble is
      probabilistic, an instance of :class:`~tfp.distributions.Distribution`. In particular, given
      the nature of the approximations stated above the final layer should be a Gaussian
      distribution with `mean` and `variance` methods.

      :param query_points: The points at which to make predictions.
      :return: The predicted mean and variance of the observations at the specified
          ``query_points``.


   .. py:method:: predict_ensemble(query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Returns mean and variance at ``query_points`` for each member of the ensemble. First tensor
      is the mean and second is the variance, where each has shape [..., M, N, 1], where M is
      the ``ensemble_size``.

      This method assumes that the final layer in each member of the ensemble is
      probabilistic, an instance of :class:`Â¬tfp.distributions.Distribution`, in particular
      `mean` and `variance` methods should be available.

      :param query_points: The points at which to make predictions.
      :return: The predicted mean and variance of the observations at the specified
          ``query_points`` for each member of the ensemble.


   .. py:method:: sample_encoded(query_points: trieste.types.TensorType, num_samples: int) -> trieste.types.TensorType

      Return ``num_samples`` samples at ``query_points``. We use the mixture approximation in
      :meth:`predict` for ``query_points`` and sample ``num_samples`` times from a Gaussian
      distribution given by the predicted mean and variance.

      :param query_points: The points at which to sample, with shape [..., N, D].
      :param num_samples: The number of samples at each point.
      :return: The samples. For a predictive distribution with event shape E, this has shape
          [..., S, N] + E, where S is the number of samples.


   .. py:method:: sample_ensemble(query_points: trieste.types.TensorType, num_samples: int) -> trieste.types.TensorType

      Return ``num_samples`` samples at ``query_points``. Each sample is taken from a Gaussian
      distribution given by the predicted mean and variance of a randomly chosen network in the
      ensemble. This avoids using the Gaussian mixture approximation and samples directly from
      individual Gaussian distributions given by each network in the ensemble.

      :param query_points: The points at which to sample, with shape [..., N, D].
      :param num_samples: The number of samples at each point.
      :return: The samples. For a predictive distribution with event shape E, this has shape
          [..., S, N] + E, where S is the number of samples.


   .. py:method:: trajectory_sampler() -> trieste.models.interfaces.TrajectorySampler[DeepEnsemble]

      Return a trajectory sampler. For :class:`DeepEnsemble`, we use an ensemble
      sampler that randomly picks a network from the ensemble and uses its predicted means
      for generating a trajectory, or optionally randomly sampled quantiles rather than means.

      :return: The trajectory sampler.


   .. py:method:: update_encoded(dataset: trieste.data.Dataset) -> None

      Neural networks are parametric models and do not need to update data.
      `TrainableProbabilisticModel` interface, however, requires an update method, so
      here we simply pass the execution.


   .. py:method:: optimize_encoded(dataset: trieste.data.Dataset) -> keras.callbacks.History

      Optimize the underlying Keras ensemble model with the specified ``dataset``.

      Optimization is performed by using the Keras `fit` method, rather than applying the
      optimizer and using the batches supplied with the optimizer wrapper. User can pass
      arguments to the `fit` method through ``minimize_args`` argument in the optimizer wrapper.
      These default to using 100 epochs, batch size 100, and verbose 0. See
      https://keras.io/api/models/model_training_apis/#fit-method for a list of possible
      arguments.

      Note that optimization does not return the result, instead optimization results are
      stored in a history attribute of the model object.

      :param dataset: The data with which to optimize the model.


   .. py:method:: log(dataset: Optional[trieste.data.Dataset] = None) -> None

      Log model training information at a given optimization step to the Tensorboard.
      We log several summary statistics of losses and metrics given in ``fit_args`` to
      ``optimizer`` (final, difference between inital and final loss, min and max). We also log
      epoch statistics, but as histograms, rather than time series. We also log several training
      data based metrics, such as root mean square error between predictions and observations,
      and several others.

      We do not log statistics of individual models in the ensemble unless specifically switched
      on with ``trieste.logging.set_summary_filter(lambda name: True)``.

      For custom logs user will need to subclass the model and overwrite this method.

      :param dataset: Optional data that can be used to log additional data-based model summaries.



