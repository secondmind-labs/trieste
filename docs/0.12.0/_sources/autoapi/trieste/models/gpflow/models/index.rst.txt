:orphan:

:py:mod:`trieste.models.gpflow.models`
======================================

.. py:module:: trieste.models.gpflow.models


Module Contents
---------------

.. py:class:: GaussianProcessRegression(model: gpflow.models.GPR, optimizer: Optimizer | None = None, num_kernel_samples: int = 10, num_rff_features: int = 1000, use_decoupled_sampler: bool = True)

   Bases: :py:obj:`trieste.models.gpflow.interface.GPflowPredictor`, :py:obj:`trieste.models.interfaces.TrainableProbabilisticModel`, :py:obj:`trieste.models.interfaces.FastUpdateModel`, :py:obj:`trieste.models.gpflow.interface.SupportsCovarianceBetweenPoints`, :py:obj:`trieste.models.interfaces.SupportsGetInternalData`, :py:obj:`trieste.models.interfaces.HasTrajectorySampler`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.GPR`.

   As Bayesian optimization requires a large number of sequential predictions (i.e. when maximizing
   acquisition functions), rather than calling the model directly at prediction time we instead
   call the posterior objects built by these models. These posterior objects store the
   pre-computed Gram matrices, which can be reused to allow faster subsequent predictions. However,
   note that these posterior objects need to be updated whenever the underlying model is changed
   by calling :meth:`update_posterior_cache` (this
   happens automatically after calls to :meth:`update` or :math:`optimize`).

   :param model: The GPflow model to wrap.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.
   :param num_kernel_samples: Number of randomly sampled kernels (for each kernel parameter) to
       evaluate before beginning model optimization. Therefore, for a kernel with `p`
       (vector-valued) parameters, we evaluate `p * num_kernel_samples` kernels.
   :param num_rff_features: The number of random Fourier features used to approximate the
       kernel when calling :meth:`trajectory_sampler`. We use a default of 1000 as it
       typically perfoms well for a wide range of kernels. Note that very smooth
       kernels (e.g. RBF) can be well-approximated with fewer features.
   :param use_decoupled_sampler: If True use a decoupled random Fourier feature sampler, else
       just use a random Fourier feature sampler. The decoupled sampler suffers less from
       overestimating variance and can typically get away with a lower num_rff_features.

   .. py:method:: model(self) -> gpflow.models.GPR
      :property:

      The underlying GPflow model.


   .. py:method:: predict_y(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Return the mean and variance of the independent marginal distributions at each point in
      ``query_points`` for the observations, including noise contributions.

      Note that this is not supported by all models.

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The mean and variance of the independent marginal distributions at each point in
          ``query_points``. For a predictive distribution with event shape E, the mean and
          variance will both have shape [...] + E.


   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.


   .. py:method:: covariance_between_points(self, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType) -> trieste.types.TensorType

      Compute the posterior covariance between sets of query points.

      .. math:: \Sigma_{12} = K_{12} - K_{x1}(K_{xx} + \sigma^2 I)^{-1}K_{x2}

      Note that query_points_2 must be a rank 2 tensor, but query_points_1 can
      have leading dimensions.

      :param query_points_1: Set of query points with shape [..., N, D]
      :param query_points_2: Sets of query points with shape [M, D]
      :return: Covariance matrix between the sets of query points with shape [..., L, N, M]
          (L being the number of latent GPs = number of output dimensions)


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      Optimize the model with the specified `dataset`.

      For :class:`GaussianProcessRegression`, we (optionally) try multiple randomly sampled
      kernel parameter configurations as well as the configuration specified when initializing
      the kernel. The best configuration is used as the starting point for model optimization.

      For trainable parameters constrained to lie in a finite interval (through a sigmoid
      bijector), we begin model optimization from the best of a random sample from these
      parameters' acceptable domains.

      For trainable parameters without constraints but with priors, we begin model optimization
      from the best of a random sample from these parameters' priors.

      For trainable parameters with neither priors nor constraints, we begin optimization from
      their initial values.

      :param dataset: The data with which to optimize the `model`.


   .. py:method:: find_best_model_initialization(self, num_kernel_samples: int) -> None

      Test `num_kernel_samples` models with sampled kernel parameters. The model's kernel
      parameters are then set to the sample achieving maximal likelihood.

      :param num_kernel_samples: Number of randomly sampled kernels to evaluate.


   .. py:method:: trajectory_sampler(self) -> trieste.models.interfaces.TrajectorySampler[GaussianProcessRegression]

      Return a trajectory sampler. For :class:`GaussianProcessRegression`, we build
      trajectories using a random Fourier feature approximation.

      At the moment only models with single latent GP are supported.

      :return: The trajectory sampler.
      :raise NotImplementedError: If we try to use the
          sampler with a model that has more than one latent GP.


   .. py:method:: get_internal_data(self) -> trieste.data.Dataset

      Return the model's training data.

      :return: The model's training data.


   .. py:method:: conditional_predict_f(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Returns the marginal GP distribution at query_points conditioned on both the model
      and some additional data, using exact formula. See :cite:`chevalier2014corrected`
      (eqs. 8-10) for details.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :return: mean_qp_new: predictive mean at query_points, with shape [..., M, L],
               and var_qp_new: predictive variance at query_points, with shape [..., M, L]


   .. py:method:: conditional_predict_joint(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Predicts the joint GP distribution at query_points conditioned on both the model
      and some additional data, using exact formula. See :cite:`chevalier2014corrected`
      (eqs. 8-10) for details.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :return: mean_qp_new: predictive mean at query_points, with shape [..., M, L],
               and cov_qp_new: predictive covariance between query_points, with shape
               [..., L, M, M]


   .. py:method:: conditional_predict_f_sample(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset, num_samples: int) -> trieste.types.TensorType

      Generates samples of the GP at query_points conditioned on both the model
      and some additional data.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :param num_samples: number of samples
      :return: samples of f at query points, with shape [..., num_samples, M, L]


   .. py:method:: conditional_predict_y(self, query_points: trieste.types.TensorType, additional_data: trieste.data.Dataset) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Generates samples of y from the GP at query_points conditioned on both the model
      and some additional data.

      :param query_points: Set of query points with shape [M, D]
      :param additional_data: Dataset with query_points with shape [..., N, D] and observations
               with shape [..., N, L]
      :return: predictive variance at query_points, with shape [..., M, L],
               and predictive variance at query_points, with shape [..., M, L]



.. py:class:: SparseGaussianProcessRegression(model: gpflow.models.SGPR, optimizer: Optimizer | None = None, num_rff_features: int = 1000, inducing_point_selector: Optional[trieste.models.gpflow.inducing_point_selectors.InducingPointSelector[SparseGaussianProcessRegression]] = None)

   Bases: :py:obj:`trieste.models.gpflow.interface.GPflowPredictor`, :py:obj:`trieste.models.interfaces.TrainableProbabilisticModel`, :py:obj:`trieste.models.gpflow.interface.SupportsCovarianceBetweenPoints`, :py:obj:`trieste.models.interfaces.SupportsGetInducingVariables`, :py:obj:`trieste.models.interfaces.SupportsGetInternalData`, :py:obj:`trieste.models.interfaces.HasTrajectorySampler`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.SGPR`.
   At the moment we only support models with a single latent GP. This is due to ``compute_qu``
   method in :class:`~gpflow.models.SGPR` that is used for computing covariance between
   query points and trajectory sampling, which at the moment works only for single latent GP.

   Similarly to our :class:`GaussianProcessRegression`, our :class:`~gpflow.models.SGPR` wrapper
   directly calls the posterior objects built by these models at prediction
   time. These posterior objects store the pre-computed Gram matrices, which can be reused to allow
   faster subsequent predictions. However, note that these posterior objects need to be updated
   whenever the underlying model is changed  by calling :meth:`update_posterior_cache` (this
   happens automatically after calls to :meth:`update` or :math:`optimize`).

   :param model: The GPflow model to wrap.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.
   :param num_rff_features: The number of random Fourier features used to approximate the
       kernel when calling :meth:`trajectory_sampler`. We use a default of 1000 as it
       typically perfoms well for a wide range of kernels. Note that very smooth
       kernels (e.g. RBF) can be well-approximated with fewer features.
   :param inducing_point_selector: The (optional) desired inducing point selector that
       will update the underlying GPflow SGPR model's inducing points as
       the optimization progresses.
   :raise NotImplementedError (or ValueError): If we try to use a model with invalid
       ``num_rff_features``, or an ``inducing_point_selector`` with a model
       that has more than one set of inducing points.

   .. py:method:: model(self) -> gpflow.models.SGPR
      :property:

      The underlying GPflow model.


   .. py:method:: predict_y(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Return the mean and variance of the independent marginal distributions at each point in
      ``query_points`` for the observations, including noise contributions.

      Note that this is not supported by all models.

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The mean and variance of the independent marginal distributions at each point in
          ``query_points``. For a predictive distribution with event shape E, the mean and
          variance will both have shape [...] + E.


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      Optimize the model with the specified `dataset`.

      :param dataset: The data with which to optimize the `model`.


   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.


   .. py:method:: _update_inducing_variables(self, new_inducing_points: trieste.types.TensorType) -> None

      When updating the inducing points of a model, we must also update the other
      inducing variables, i.e. `q_mu` and `q_sqrt` accordingly. The exact form of this update
      depends if we are using whitened representations of the inducing variables.
      See :meth:`_whiten_points` for details.

      :param new_inducing_points: The desired values for the new inducing points.
      :raise NotImplementedError: If we try to update the inducing variables of a model
          that has more than one set of inducing points.


   .. py:method:: get_inducing_variables(self) -> Tuple[Union[trieste.types.TensorType, list[trieste.types.TensorType]], trieste.types.TensorType, trieste.types.TensorType, bool]

      Return the model's inducing variables. The SGPR model does not have ``q_mu``, ``q_sqrt`` and
      ``whiten`` objects. We can use ``compute_qu`` method to obtain ``q_mu`` and ``q_sqrt``,
      while the SGPR model does not use the whitened representation. Note that at the moment
      ``compute_qu`` works only for single latent GP and returns ``q_sqrt`` in a shape that is
      inconsistent with the SVGP model (hence we need to do modify its shape).

      :return: The inducing points (i.e. locations of the inducing variables), as a Tensor or a
          list of Tensors (when the model has multiple inducing points); a tensor containing the
          variational mean ``q_mu``; a tensor containing the Cholesky decomposition of the
          variational covariance ``q_sqrt``; and a bool denoting if we are using whitened or
          non-whitened representations.
      :raise NotImplementedError: If the model has more than one latent GP.


   .. py:method:: covariance_between_points(self, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType) -> trieste.types.TensorType

      Compute the posterior covariance between sets of query points.

      Note that query_points_2 must be a rank 2 tensor, but query_points_1 can
      have leading dimensions.

      :param query_points_1: Set of query points with shape [..., A, D]
      :param query_points_2: Sets of query points with shape [B, D]
      :return: Covariance matrix between the sets of query points with shape [..., L, A, B]
          (L being the number of latent GPs = number of output dimensions)


   .. py:method:: trajectory_sampler(self) -> trieste.models.interfaces.TrajectorySampler[SparseGaussianProcessRegression]

      Return a trajectory sampler. For :class:`SparseGaussianProcessRegression`, we build
      trajectories using a decoupled random Fourier feature approximation. Note that this
      is available only for single output models.

      At the moment only models with single latent GP are supported.

      :return: The trajectory sampler.
      :raise NotImplementedError: If we try to use the
          sampler with a model that has more than one latent GP.


   .. py:method:: get_internal_data(self) -> trieste.data.Dataset

      Return the model's training data.

      :return: The model's training data.



.. py:class:: SparseVariational(model: gpflow.models.SVGP, optimizer: Optimizer | None = None, num_rff_features: int = 1000, inducing_point_selector: Optional[trieste.models.gpflow.inducing_point_selectors.InducingPointSelector[SparseVariational]] = None)

   Bases: :py:obj:`trieste.models.gpflow.interface.GPflowPredictor`, :py:obj:`trieste.models.interfaces.TrainableProbabilisticModel`, :py:obj:`trieste.models.gpflow.interface.SupportsCovarianceBetweenPoints`, :py:obj:`trieste.models.interfaces.SupportsGetInducingVariables`, :py:obj:`trieste.models.interfaces.HasTrajectorySampler`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.SVGP`.

   Similarly to our :class:`GaussianProcessRegression`, our :class:`~gpflow.models.SVGP` wrapper
   directly calls the posterior objects built by these models at prediction
   time. These posterior objects store the pre-computed Gram matrices, which can be reused to allow
   faster subsequent predictions. However, note that these posterior objects need to be updated
   whenever the underlying model is changed  by calling :meth:`update_posterior_cache` (this
   happens automatically after calls to :meth:`update` or :math:`optimize`).

   :param model: The underlying GPflow sparse variational model.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.BatchOptimizer` with :class:`~tf.optimizers.Adam` with
       batch size 100.
   :param num_rff_features: The number of random Fourier features used to approximate the
       kernel when performing decoupled Thompson sampling through
       its :meth:`trajectory_sampler`. We use a default of 1000 as it typically
       perfoms well for a wide range of kernels. Note that very smooth kernels (e.g. RBF)
       can be well-approximated with fewer features.
   :param inducing_point_selector: The (optional) desired inducing_point_selector that
       will update the underlying GPflow sparse variational model's inducing points as
       the optimization progresses.
   :raise NotImplementedError: If we try to use an inducing_point_selector with a model
       that has more than one set of inducing points.

   .. py:method:: model(self) -> gpflow.models.SVGP
      :property:

      The underlying GPflow model.


   .. py:method:: predict_y(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Return the mean and variance of the independent marginal distributions at each point in
      ``query_points`` for the observations, including noise contributions.

      Note that this is not supported by all models.

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The mean and variance of the independent marginal distributions at each point in
          ``query_points``. For a predictive distribution with event shape E, the mean and
          variance will both have shape [...] + E.


   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      Optimize the model with the specified `dataset`.

      :param dataset: The data with which to optimize the `model`.


   .. py:method:: _update_inducing_variables(self, new_inducing_points: trieste.types.TensorType) -> None

      When updating the inducing points of a model, we must also update the other
      inducing variables, i.e. `q_mu` and `q_sqrt` accordingly. The exact form of this update
      depends if we are using whitened representations of the inducing variables.
      See :meth:`_whiten_points` for details.

      :param new_inducing_points: The desired values for the new inducing points.
      :raise NotImplementedError: If we try to update the inducing variables of a model
          that has more than one set of inducing points.


   .. py:method:: get_inducing_variables(self) -> Tuple[Union[trieste.types.TensorType, list[trieste.types.TensorType]], trieste.types.TensorType, trieste.types.TensorType, bool]

      Return the model's inducing variables.

      :return: The inducing points (i.e. locations of the inducing variables), as a Tensor or a
          list of Tensors (when the model has multiple inducing points); A tensor containing the
          variational mean q_mu; a tensor containing the Cholesky decomposition of the variational
          covariance q_sqrt; and a bool denoting if we are using whitened or
          non-whitened representations.


   .. py:method:: covariance_between_points(self, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType) -> trieste.types.TensorType

      Compute the posterior covariance between sets of query points.

      Note that query_points_2 must be a rank 2 tensor, but query_points_1 can
      have leading dimensions.

      :param query_points_1: Set of query points with shape [..., A, D]
      :param query_points_2: Sets of query points with shape [B, D]
      :return: Covariance matrix between the sets of query points with shape [..., L, A, B]
          (L being the number of latent GPs = number of output dimensions)


   .. py:method:: trajectory_sampler(self) -> trieste.models.interfaces.TrajectorySampler[SparseVariational]

      Return a trajectory sampler. For :class:`SparseVariational`, we build
      trajectories using a decoupled random Fourier feature approximation.

      At the moment only models with single latent GP are supported.

      :return: The trajectory sampler.
      :raise NotImplementedError: If we try to use the
          sampler with a model that has more than one latent GP.



.. py:class:: VariationalGaussianProcess(model: gpflow.models.VGP, optimizer: Optimizer | None = None, use_natgrads: bool = False, natgrad_gamma: Optional[float] = None, num_rff_features: int = 1000)

   Bases: :py:obj:`trieste.models.gpflow.interface.GPflowPredictor`, :py:obj:`trieste.models.interfaces.TrainableProbabilisticModel`, :py:obj:`trieste.models.gpflow.interface.SupportsCovarianceBetweenPoints`, :py:obj:`trieste.models.interfaces.SupportsGetInducingVariables`, :py:obj:`trieste.models.interfaces.HasTrajectorySampler`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.VGP`.

   A Variational Gaussian Process (VGP) approximates the posterior of a GP
   using the multivariate Gaussian closest to the posterior of the GP by minimizing the
   KL divergence between approximated and exact posteriors. See :cite:`opper2009variational`
   for details.

   The VGP provides (approximate) GP modelling under non-Gaussian likelihoods, for example
   when fitting a classification model over binary data.

   A whitened representation and (optional) natural gradient steps are used to aid
   model optimization.

   Similarly to our :class:`GaussianProcessRegression`, our :class:`~gpflow.models.VGP` wrapper
   directly calls the posterior objects built by these models at prediction
   time. These posterior objects store the pre-computed Gram matrices, which can be reused to allow
   faster subsequent predictions. However, note that these posterior objects need to be updated
   whenever the underlying model is changed  by calling :meth:`update_posterior_cache` (this
   happens automatically after calls to :meth:`update` or :math:`optimize`).

   :param model: The GPflow :class:`~gpflow.models.VGP`.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.
   :param use_natgrads: If True then alternate model optimization steps with natural
       gradient updates. Note that natural gradients requires
       a :class:`~trieste.models.optimizer.BatchOptimizer` wrapper with
       :class:`~tf.optimizers.Optimizer` optimizer.
   :natgrad_gamma: Gamma parameter for the natural gradient optimizer.
   :param num_rff_features: The number of random Fourier features used to approximate the
       kernel when performing decoupled Thompson sampling through
       its :meth:`trajectory_sampler`. We use a default of 1000 as it typically perfoms
       well for a wide range of kernels. Note that very smooth kernels (e.g. RBF) can
       be well-approximated with fewer features.
   :raise ValueError (or InvalidArgumentError): If ``model``'s :attr:`q_sqrt` is not rank 3
       or if attempting to combine natural gradients with a :class:`~gpflow.optimizers.Scipy`
       optimizer.

   .. py:method:: model(self) -> gpflow.models.VGP
      :property:

      The underlying GPflow model.


   .. py:method:: predict_y(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      Return the mean and variance of the independent marginal distributions at each point in
      ``query_points`` for the observations, including noise contributions.

      Note that this is not supported by all models.

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The mean and variance of the independent marginal distributions at each point in
          ``query_points``. For a predictive distribution with event shape E, the mean and
          variance will both have shape [...] + E.


   .. py:method:: update(self, dataset: trieste.data.Dataset, *, jitter: float = DEFAULTS.JITTER) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.
      :param jitter: The size of the jitter to use when stabilizing the Cholesky decomposition of
          the covariance matrix.


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      :class:`VariationalGaussianProcess` has a custom `optimize` method that (optionally) permits
      alternating between standard optimization steps (for kernel parameters) and natural gradient
      steps for the variational parameters (`q_mu` and `q_sqrt`). See :cite:`salimbeni2018natural`
      for details. Using natural gradients can dramatically speed up model fitting, especially for
      ill-conditioned posteriors.

      If using natural gradients, our optimizer inherits the mini-batch behavior and number
      of optimization steps as the base optimizer specified when initializing
      the :class:`VariationalGaussianProcess`.


   .. py:method:: get_inducing_variables(self) -> Tuple[trieste.types.TensorType, trieste.types.TensorType, trieste.types.TensorType, bool]

      Return the model's inducing variables. Note that GPflow's VGP model is
      hard-coded to use the whitened representation.

      :return: Tensors containing: the inducing points (i.e. locations of the inducing
          variables); the variational mean q_mu; the Cholesky decomposition of the
          variational covariance q_sqrt; and a bool denoting if we are using whitened
          or non-whitened representations.


   .. py:method:: trajectory_sampler(self) -> trieste.models.interfaces.TrajectorySampler[VariationalGaussianProcess]

      Return a trajectory sampler. For :class:`VariationalGaussianProcess`, we build
      trajectories using a decoupled random Fourier feature approximation.

      At the moment only models with single latent GP are supported.

      :return: The trajectory sampler.
      :raise NotImplementedError: If we try to use the
          sampler with a model that has more than one latent GP.


   .. py:method:: covariance_between_points(self, query_points_1: trieste.types.TensorType, query_points_2: trieste.types.TensorType) -> trieste.types.TensorType

      Compute the posterior covariance between sets of query points.

      Note that query_points_2 must be a rank 2 tensor, but query_points_1 can
      have leading dimensions.

      :param query_points_1: Set of query points with shape [..., A, D]
      :param query_points_2: Sets of query points with shape [B, D]
      :return: Covariance matrix between the sets of query points with shape [..., L, A, B]
          (L being the number of latent GPs = number of output dimensions)



