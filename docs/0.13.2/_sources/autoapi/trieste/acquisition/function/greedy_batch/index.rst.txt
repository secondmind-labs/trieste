:py:mod:`trieste.acquisition.function.greedy_batch`
===================================================

.. py:module:: trieste.acquisition.function.greedy_batch

.. autoapi-nested-parse::

   This module contains local penalization-based acquisition function builders.



Module Contents
---------------

.. py:class:: LocalPenalization(search_space: trieste.space.SearchSpace, num_samples: int = 500, penalizer: Callable[[trieste.models.ProbabilisticModel, trieste.types.TensorType, trieste.types.TensorType, trieste.types.TensorType], Union[trieste.acquisition.interface.PenalizationFunction, trieste.acquisition.interface.UpdatablePenalizationFunction]] = None, base_acquisition_function_builder: ExpectedImprovement | MinValueEntropySearch[ProbabilisticModel] | MakePositive[ProbabilisticModel] | None = None)

   Bases: :py:obj:`trieste.acquisition.interface.SingleModelGreedyAcquisitionBuilder`\ [\ :py:obj:`trieste.models.ProbabilisticModel`\ ]

   Builder of the acquisition function maker for greedily collecting batches by local
   penalization.  The resulting :const:`AcquisitionFunctionMaker` takes in a set of pending
   points and returns a base acquisition function penalized around those points.
   An estimate of the objective function's Lipschitz constant is used to control the size
   of penalization.

   Local penalization allows us to perform batch Bayesian optimization with a standard (non-batch)
   acquisition function. All that we require is that the acquisition function takes strictly
   positive values. By iteratively building a batch of points though sequentially maximizing
   this acquisition function but down-weighted around locations close to the already
   chosen (pending) points, local penalization provides diverse batches of candidate points.

   Local penalization is applied to the acquisition function multiplicatively. However, to
   improve numerical stability, we perform additive penalization in a log space.

   The Lipschitz constant and additional penalization parameters are estimated once
   when first preparing the acquisition function with no pending points. These estimates
   are reused for all subsequent function calls.

   :param search_space: The global search space over which the optimisation is defined.
   :param num_samples: Size of the random sample over which the Lipschitz constant
       is estimated. We recommend scaling this with search space dimension.
   :param penalizer: The chosen penalization method (defaults to soft penalization). This
       should be a function that accepts a model, pending points, lipschitz constant and eta
       and returns a PenalizationFunction.
   :param base_acquisition_function_builder: Base acquisition function to be
       penalized (defaults to expected improvement). Local penalization only supports
       strictly positive acquisition functions.
   :raise tf.errors.InvalidArgumentError: If ``num_samples`` is not positive.

   .. py:method:: prepare_acquisition_function(self, model: trieste.models.ProbabilisticModel, dataset: Optional[trieste.data.Dataset] = None, pending_points: Optional[trieste.types.TensorType] = None) -> trieste.acquisition.interface.AcquisitionFunction

      :param model: The model.
      :param dataset: The data from the observer. Must be populated.
      :param pending_points: The points we penalize with respect to.
      :return: The (log) expected improvement penalized with respect to the pending points.
      :raise tf.errors.InvalidArgumentError: If the ``dataset`` is empty.


   .. py:method:: update_acquisition_function(self, function: trieste.acquisition.interface.AcquisitionFunction, model: trieste.models.ProbabilisticModel, dataset: Optional[trieste.data.Dataset] = None, pending_points: Optional[trieste.types.TensorType] = None, new_optimization_step: bool = True) -> trieste.acquisition.interface.AcquisitionFunction

      :param function: The acquisition function to update.
      :param model: The model.
      :param dataset: The data from the observer. Must be populated.
      :param pending_points: Points already chosen to be in the current batch (of shape [M,D]),
          where M is the number of pending points and D is the search space dimension.
      :param new_optimization_step: Indicates whether this call to update_acquisition_function
          is to start of a new optimization step, of to continue collecting batch of points
          for the current step. Defaults to ``True``.
      :return: The updated acquisition function.



.. py:class:: PenalizedAcquisition(base_acquisition_function: trieste.acquisition.interface.AcquisitionFunction, penalization: trieste.acquisition.interface.PenalizationFunction)

   Class representing a penalized acquisition function.

   :param base_acquisition_function: Base (unpenalized) acquisition function.
   :param penalization: Penalization function.


.. py:class:: local_penalizer(model: trieste.models.ProbabilisticModel, pending_points: trieste.types.TensorType, lipschitz_constant: trieste.types.TensorType, eta: trieste.types.TensorType)

   Bases: :py:obj:`trieste.acquisition.interface.UpdatablePenalizationFunction`

   An :class:`UpdatablePenalizationFunction` builds and updates a penalization function.
   Defining a penalization function that can be updated avoids having to retrace on every call.

   Initialize the local penalizer.

   :param model: The model over the specified ``dataset``.
   :param pending_points: The points we penalize with respect to.
   :param lipschitz_constant: The estimated Lipschitz constant of the objective function.
   :param eta: The estimated global minima.
   :return: The local penalization function. This function will raise
       :exc:`ValueError` or :exc:`~tf.errors.InvalidArgumentError` if used with a batch size
       greater than one.

   .. py:method:: update(self, pending_points: trieste.types.TensorType, lipschitz_constant: trieste.types.TensorType, eta: trieste.types.TensorType) -> None

      Update the local penalizer with new variable values.



.. py:class:: soft_local_penalizer(model: trieste.models.ProbabilisticModel, pending_points: trieste.types.TensorType, lipschitz_constant: trieste.types.TensorType, eta: trieste.types.TensorType)

   Bases: :py:obj:`local_penalizer`

   Return the soft local penalization function used for single-objective greedy batch Bayesian
   optimization in :cite:`Gonzalez:2016`.

   Soft penalization returns the probability that a candidate point does not belong
   in the exclusion zones of the pending points. For model posterior mean :math:`\mu`, model
   posterior variance :math:`\sigma^2`, current "best" function value :math:`\eta`, and an
   estimated Lipschitz constant :math:`L`,the penalization from a set of pending point
   :math:`x'` on a candidate point :math:`x` is given by
   .. math:: \phi(x, x') = \frac{1}{2}\textrm{erfc}(-z)
   where :math:`z = \frac{1}{\sqrt{2\sigma^2(x')}}(L||x'-x|| + \eta - \mu(x'))`.

   The penalization from a set of pending points is just product of the individual
   penalizations. See :cite:`Gonzalez:2016` for a full derivation.

   :param model: The model over the specified ``dataset``.
   :param pending_points: The points we penalize with respect to.
   :param lipschitz_constant: The estimated Lipschitz constant of the objective function.
   :param eta: The estimated global minima.
   :return: The local penalization function. This function will raise
       :exc:`ValueError` or :exc:`~tf.errors.InvalidArgumentError` if used with a batch size
       greater than one.

   Initialize the local penalizer.

   :param model: The model over the specified ``dataset``.
   :param pending_points: The points we penalize with respect to.
   :param lipschitz_constant: The estimated Lipschitz constant of the objective function.
   :param eta: The estimated global minima.
   :return: The local penalization function. This function will raise
       :exc:`ValueError` or :exc:`~tf.errors.InvalidArgumentError` if used with a batch size
       greater than one.

   .. py:method:: __call__(self, x: trieste.types.TensorType) -> trieste.types.TensorType

      Call penalization function..



.. py:class:: hard_local_penalizer(model: trieste.models.ProbabilisticModel, pending_points: trieste.types.TensorType, lipschitz_constant: trieste.types.TensorType, eta: trieste.types.TensorType)

   Bases: :py:obj:`local_penalizer`

   Return the hard local penalization function used for single-objective greedy batch Bayesian
   optimization in :cite:`Alvi:2019`.

   Hard penalization is a stronger penalizer than soft penalization and is sometimes more effective
   See :cite:`Alvi:2019` for details. Our implementation follows theirs, with the penalization from
   a set of pending points being the product of the individual penalizations.

   :param model: The model over the specified ``dataset``.
   :param pending_points: The points we penalize with respect to.
   :param lipschitz_constant: The estimated Lipschitz constant of the objective function.
   :param eta: The estimated global minima.
   :return: The local penalization function. This function will raise
       :exc:`ValueError` or :exc:`~tf.errors.InvalidArgumentError` if used with a batch size
       greater than one.

   Initialize the local penalizer.

   :param model: The model over the specified ``dataset``.
   :param pending_points: The points we penalize with respect to.
   :param lipschitz_constant: The estimated Lipschitz constant of the objective function.
   :param eta: The estimated global minima.
   :return: The local penalization function. This function will raise
       :exc:`ValueError` or :exc:`~tf.errors.InvalidArgumentError` if used with a batch size
       greater than one.

   .. py:method:: __call__(self, x: trieste.types.TensorType) -> trieste.types.TensorType

      Call penalization function..



.. py:class:: FantasizerModelType

   Bases: :py:obj:`trieste.models.FastUpdateModel`, :py:obj:`trieste.models.interfaces.SupportsPredictJoint`, :py:obj:`trieste.models.interfaces.SupportsGetKernel`, :py:obj:`trieste.models.interfaces.SupportsGetObservationNoise`, :py:obj:`typing_extensions.Protocol`

   The model requirements for the Fantasizer acquisition function.


.. py:class:: FantasizerModelStack

   Bases: :py:obj:`trieste.models.interfaces.PredictJointModelStack`, :py:obj:`trieste.models.ModelStack`\ [\ :py:obj:`FantasizerModelType`\ ]

   A stack of models :class:`FantasizerModelType` models. Note that this delegates predict_joint
   but none of the other methods.

   Initialize self.  See help(type(self)) for accurate signature.


.. py:class:: Fantasizer(base_acquisition_function_builder: Optional[AcquisitionFunctionBuilder[SupportsPredictJoint] | SingleModelAcquisitionBuilder[SupportsPredictJoint]] = None, fantasize_method: str = 'KB')

   Bases: :py:obj:`trieste.acquisition.interface.GreedyAcquisitionFunctionBuilder`\ [\ :py:obj:`FantasizerModelOrStack`\ ]

   Builder of the acquisition function maker for greedily collecting batches.
   Fantasizer allows us to perform batch Bayesian optimization with any
   standard (non-batch) acquisition function.

   Here, every time a query point is chosen by maximising an acquisition function,
   its corresponding observation is "fantasized", and the models are conditioned further
   on this new artificial data.

   This implies that the models need to predict what their updated predictions would be given
   new data, see :class:`~FastUpdateModel`. These equations are for instance in closed form
   for the GPR model, see :cite:`chevalier2014corrected` (eqs. 8-10) for details.

   There are several ways to "fantasize" data: the "kriging believer" heuristic (KB, see
   :cite:`ginsbourger2010kriging`) uses the mean of the model as observations.
   "sample" uses samples from the model.

   :param base_acquisition_function_builder: The acquisition function builder to use.
       Defaults to :class:`~trieste.acquisition.ExpectedImprovement`.
   :param fantasize_method: The following options are available: "KB" and "sample".
       See class docs for more details.
   :raise tf.errors.InvalidArgumentError: If ``fantasize_method`` is not "KB" or "sample".

   .. py:method:: prepare_acquisition_function(self, models: Mapping[str, FantasizerModelOrStack], datasets: Optional[Mapping[str, trieste.data.Dataset]] = None, pending_points: Optional[trieste.types.TensorType] = None) -> trieste.acquisition.interface.AcquisitionFunction

      :param models: The models over each tag.
      :param datasets: The data from the observer (optional).
      :param pending_points: Points already chosen to be in the current batch (of shape [M,D]),
          where M is the number of pending points and D is the search space dimension.
      :return: An acquisition function.


   .. py:method:: update_acquisition_function(self, function: trieste.acquisition.interface.AcquisitionFunction, models: Mapping[str, FantasizerModelOrStack], datasets: Optional[Mapping[str, trieste.data.Dataset]] = None, pending_points: Optional[trieste.types.TensorType] = None, new_optimization_step: bool = True) -> trieste.acquisition.interface.AcquisitionFunction

      :param function: The acquisition function to update.
      :param models: The models over each tag.
      :param datasets: The data from the observer (optional).
      :param pending_points: Points already chosen to be in the current batch (of shape [M,D]),
          where M is the number of pending points and D is the search space dimension.
      :param new_optimization_step: Indicates whether this call to update_acquisition_function
          is to start of a new optimization step, of to continue collecting batch of points
          for the current step. Defaults to ``True``.
      :return: The updated acquisition function.



.. py:function:: _generate_fantasized_data(fantasize_method: str, model: FantasizerModelOrStack, pending_points: trieste.types.TensorType) -> trieste.data.Dataset

   Generates "fantasized" data at pending_points depending on the chosen heuristic:
   - KB (kriging believer) uses the mean prediction of the models
   - sample uses samples from the GP posterior.

   :param fantasize_method: the following options are available: "KB" and "sample".
   :param model: a model with predict method
   :param dataset: past data
   :param pending_points: points at which to fantasize data
   :return: a fantasized dataset


.. py:class:: _fantasized_model(model: FantasizerModelType, fantasized_data: trieste.data.Dataset)

   Bases: :py:obj:`trieste.models.interfaces.SupportsPredictJoint`, :py:obj:`trieste.models.interfaces.SupportsGetKernel`, :py:obj:`trieste.models.interfaces.SupportsGetObservationNoise`

   Creates a new model from an existing one and additional data.
   This new model posterior is conditioned on both current model data and the additional one.

   :param model: a model, must be of class `FastUpdateModel`
   :param fantasized_data: additional dataset to condition on
   :raise NotImplementedError: If model is not of class `FastUpdateModel`.

   .. py:method:: update_fantasized_data(self, fantasized_data: trieste.data.Dataset) -> None

      :param fantasized_data: new additional dataset to condition on


   .. py:method:: predict(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      This function wraps conditional_predict_f. It cannot directly call
      conditional_predict_f, since it does not accept query_points with rank > 2.
      We use map_fn to allow leading dimensions for query_points.

      :param query_points: shape [...*, N, d]
      :return: mean, shape [...*, ..., N, L] and cov, shape [...*, ..., L, N],
          where ... are the leading dimensions of fantasized_data


   .. py:method:: predict_joint(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      This function wraps conditional_predict_joint. It cannot directly call
      conditional_predict_joint, since it does not accept query_points with rank > 2.
      We use map_fn to allow leading dimensions for query_points.

      :param query_points: shape [...*, N, D]
      :return: mean, shape [...*, ..., N, L] and cov, shape [...*, ..., L, N, N],
          where ... are the leading dimensions of fantasized_data


   .. py:method:: sample(self, query_points: trieste.types.TensorType, num_samples: int) -> trieste.types.TensorType

      This function wraps conditional_predict_f_sample. It cannot directly call
      conditional_predict_joint, since it does not accept query_points with rank > 2.
      We use map_fn to allow leading dimensions for query_points.

      :param query_points: shape [...*, N, D]
      :param num_samples: number of samples.
      :return: samples of shape [...*, ..., S, N, L], where ... are the leading
          dimensions of fantasized_data


   .. py:method:: predict_y(self, query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

      This function wraps conditional_predict_y. It cannot directly call
      conditional_predict_joint, since it does not accept query_points with rank > 2.
      We use tf.map_fn to allow leading dimensions for query_points.

      :param query_points: shape [...*, N, D]
      :return: mean, shape [...*, ..., N, L] and var, shape [...*, ..., L, N],
          where ... are the leading dimensions of fantasized_data


   .. py:method:: get_observation_noise(self) -> trieste.types.TensorType

      Return the variance of observation noise.

      :return: The observation noise.


   .. py:method:: get_kernel(self) -> gpflow.kernels.Kernel

      Return the kernel of the model.
      :return: The kernel.


   .. py:method:: log(self, dataset: Optional[trieste.data.Dataset] = None) -> None

      Log model-specific information at a given optimization step.

      :param dataset: Optional data that can be used to log additional data-based model summaries.



.. py:function:: _broadcast_predict(query_points: trieste.types.TensorType, fun: Callable[[trieste.types.TensorType], tuple[trieste.types.TensorType, trieste.types.TensorType]]) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

   Utility function that allows leading dimensions for query_points when
   fun only accepts rank 2 tensors. It works by flattening query_points into
   a rank 3 tensor, evaluate fun(query_points) through tf.map_fn, then
   restoring the leading dimensions.

   :param query_points: shape [...*, N, D]
   :param fun: callable that returns two tensors (e.g. a predict function)
   :return: two tensors (e.g. mean and variance) with shape [...*, ...]


.. py:function:: _get_leading_dim_and_flatten(query_points: trieste.types.TensorType) -> tuple[trieste.types.TensorType, trieste.types.TensorType]

   :param query_points: shape [...*, N, D]
   :return: leading_dim = ....*, query_points_flatten, shape [B, N, D]


.. py:function:: _restore_leading_dim(x: trieste.types.TensorType, leading_dim: trieste.types.TensorType) -> trieste.types.TensorType

   "Un-flatten" the first dimension of x to leading_dim

   :param x: shape [B, ...]
   :param leading_dim: [...*]
   :return: shape [...*, ...]


